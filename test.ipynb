{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Sample from pcap(already converted in CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define set of hyper-parameters\n",
    "List of tuples (duration, overlap)\n",
    "'''\n",
    "params = [(5, 3), (4, 2), (3, 1), (2, 0), (1, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/Users/leonardoscappatura/Downloads/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_folder = os.path.join(root_path, 'sources')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for duration, overlap in params:\n",
    "    # folder that contain samples of one set of parameters\n",
    "    param_folder = os.path.join(root_path, '%d_%d'%(duration, overlap))\n",
    "\n",
    "    # check whether the data is already generated or not\n",
    "    if not os.path.exists(param_folder):\n",
    "        os.mkdir(param_folder)\n",
    "\n",
    "        # create folder to contain samples\n",
    "        samples_folder = os.path.join(param_folder, 'samples')\n",
    "        os.mkdir(samples_folder)\n",
    "\n",
    "        # loop over each app to generate samples\n",
    "        for app in os.listdir(sources_folder):\n",
    "            if app == '.DS_Store':\n",
    "                continue\n",
    "\n",
    "            print('App: ', app)\n",
    "            app_sources_folder = os.path.join(sources_folder, app)\n",
    "\n",
    "\n",
    "            # create folder contain samples for each app\n",
    "            app_samples_folder = os.path.join(samples_folder, app)\n",
    "            if not os.path.exists(app_samples_folder):\n",
    "                os.mkdir(app_samples_folder)\n",
    "\n",
    "\n",
    "                for filename in os.listdir(app_sources_folder):\n",
    "                    if filename == '.DS_Store':\n",
    "                        continue\n",
    "\n",
    "                    print('Processing %s ...' % filename)\n",
    "                    index = 1\n",
    "\n",
    "\n",
    "                    file_path = os.path.join(app_sources_folder, filename)\n",
    "                    df = pd.read_csv(file_path, index_col=0)\n",
    "                    base = df['Time'].iloc[0]\n",
    "                    end = df['Time'].iloc[-1]\n",
    "      \n",
    "                    while ((index - 1)*(duration - overlap) + duration)*60 + base < end:\n",
    "                        start_time = base + (index-1)*(duration - overlap)*60\n",
    "                        end_time = start_time + duration*60\n",
    "                        df_ = df[(df['Time'] >= start_time) & (df['Time'] <= end_time)].reset_index(drop=True)\n",
    "\n",
    "                        # save a sample as csv file\n",
    "                        sample_filename = filename + '_' + str(index) + '.csv'\n",
    "                        sample_path = os.path.join(app_samples_folder, sample_filename)\n",
    "                        df_.to_csv(sample_path, index=True)\n",
    "\n",
    "                        index += 1\n",
    "        print('...................................................')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define set of hyper-parameters\n",
    "List of tuples (duration, overlap)\n",
    "'''\n",
    "params = [(5, 3), (4, 2), (3, 1), (2, 0), (1, 0)]\n",
    "\n",
    "train_size = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input: list of filenames (samples) and the train size\n",
    "output: list of filenames for training, list of filenames of testing\n",
    "\"\"\"\n",
    "\n",
    "def get_train_test(filenames, train_size):\n",
    "  train_idx = random.sample(range(len(filenames)), int(len(filenames)*train_size))\n",
    "  train_filenames = [filenames[i] for i in train_idx]\n",
    "\n",
    "  test_filenames = list(set(filenames) - set(train_filenames))\n",
    "\n",
    "  return (train_filenames, test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over set of hyper-parameters\n",
    "for duration, overlap in params:\n",
    "  \n",
    "  param_folder = os.path.join(root_path, '%d_%d'%(duration, overlap))\n",
    "  samples_folder = os.path.join(param_folder, 'samples')\n",
    "\n",
    "  # initial a dictionary containing training and testing information\n",
    "  train_test_info = dict()\n",
    "  for app in os.listdir(samples_folder):\n",
    "    if app == '.DS_Store':\n",
    "      continue\n",
    "    app_folder = os.path.join(samples_folder, app)\n",
    "    filenames = os.listdir(app_folder)\n",
    "\n",
    "    train_test_info[app] = get_train_test(filenames, train_size)\n",
    "\n",
    "  # save train_test_info as json file\n",
    "  saved_path = os.path.join(param_folder, 'train_test_info.json')\n",
    "  with open(saved_path, 'w') as f:\n",
    "    json.dump(train_test_info, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this path depends on your setup (need to contain sources folder)\n",
    "root_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 1\n",
    "overlap = 0\n",
    "N = 20\n",
    "window = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph generator functions - All def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_reprocessing(df, N):\n",
    "\n",
    "  # remove dns protocol\n",
    "  df = df[(df['source_port'] != 53) & (df['destination_port'] != 53) & \n",
    "        (df['source_port'] != 5353) & (df['destination_port'] != 5353) &\n",
    "        (df['source_port'] != 137) & (df['destination_port'] != 137) &\n",
    "        (df['source_port'] != 67) & (df['destination_port'] != 67) &\n",
    "        (df['source_port'] != 68) & (df['destination_port'] != 68) &\n",
    "        (df['source_port'] != 5355) & (df['destination_port'] != 5355)]\n",
    "  \n",
    "  # get IP address and port number of the service\n",
    "  df['des_greater_src'] = df['destination_port'] - df['source_port']\n",
    "  df1 = df[df['des_greater_src'] > 0]\n",
    "  df2 = df[df['des_greater_src'] < 0]\n",
    "  df1['destination'] = df1['source_address']\n",
    "  df1['port'] = df1['source_port']\n",
    "  df1['outgoing'] = 0\n",
    "  df2['destination'] = df2['destination_address']\n",
    "  df2['port'] = df2['destination_port']\n",
    "  df2['outgoing'] = 1\n",
    "  df = pd.concat([df1, df2], ignore_index=True).sort_values(by='Time').reset_index(drop=True)\n",
    "\n",
    "  # merge IP address into port (same tuple (IP, port) - same network destination)\n",
    "  df['IP_port'] = list(zip(df['destination'], df['port']))\n",
    "\n",
    "  df = df.drop(['source_address', 'destination_address', 'certificate', 'des_greater_src', 'source_port', 'destination_port', 'destination', 'port'], axis=1)\n",
    "\n",
    "  # get N network destinations that have the most packets\n",
    "  df_ = df.groupby(['IP_port'], as_index = False).agg({'length':['count']}).sort_values(by=[('length', 'count')], ascending=False)\n",
    "  destinations = df_[:N]['IP_port']\n",
    "\n",
    "  return df[df['IP_port'].isin(destinations)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packet-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pkt_reprocessing(df):\n",
    "  df = df.drop(['Time', 'stream_id', 'protocol'], axis=1).reset_index(drop=True)\n",
    "  df = df.sort_values(by=['IP_port']).reset_index(drop=True)\n",
    "\n",
    "  # return 3 series of packet: outgoing, incoming, both\n",
    "  out_df = df[df['outgoing'] == 1].drop(['outgoing'], axis=1).reset_index(drop=True)\n",
    "  in_df = df[df['outgoing'] == 0].drop(['outgoing'], axis=1).reset_index(drop=True)\n",
    "  full_df = df.drop(['outgoing'], axis=1)\n",
    "\n",
    "  return out_df, in_df, full_df\n",
    "\n",
    "def percentile(n):\n",
    "    def percentile_(x):\n",
    "        return np.percentile(x, n)\n",
    "    percentile_.__name__ = 'percentile_%s' % n\n",
    "    return percentile_\n",
    "\n",
    "def extract_pkt_features(df, type=\"complete\"):\n",
    "  features_df = df.groupby(['IP_port'], as_index = False).\\\n",
    "    agg({'length':['max', 'min', 'mean', 'mad', 'std', 'var', 'skew', pd.DataFrame.kurt, 'count', \n",
    "                   percentile(10), percentile(20), percentile(30), percentile(40), percentile(50), \n",
    "                   percentile(60), percentile(70), percentile(80), percentile(90)],\n",
    "     })\n",
    "    \n",
    "  # rename columns\n",
    "  feature_names = ['max', 'min', 'mean', 'mad', 'std', 'var', 'skew', 'kurt', 'pkt_num', '10per', '20per', '30per', '40per', '50per', '60per', '70per', '80per', '90per']\n",
    "  features_df.columns = ['IP_port'] + [type + \"_\" + x for x in feature_names]\n",
    "\n",
    "    \n",
    "  return features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flow-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_reprocessing(df):\n",
    "\n",
    "  df['protocol'] = df['protocol'] == 'tcp'\n",
    "  df['protocol'] = df['protocol'].astype('int')\n",
    "\n",
    "  # sort by stream_id, protocol, Time\n",
    "  df = df.sort_values(by=['stream_id', 'protocol', 'Time']).reset_index(drop=True)\n",
    "\n",
    "  # merge packets into flows\n",
    "  df =  df.groupby(['stream_id', 'protocol', 'IP_port'], as_index = False).\\\n",
    "              agg({'Time':['min', 'max'],\n",
    "                    'length':['sum', 'count']})\n",
    "  \n",
    "  df = df.drop(['stream_id'], axis=1)\n",
    "\n",
    "  df.columns = ['protocol', 'IP_port', 'start', 'end', 'flow_length', 'pkt_num']\n",
    "  \n",
    "  # create duration of each flow\n",
    "  df['duration'] = df['end'] - df['start']\n",
    "  df = df.drop(['end', 'start'], axis=1)\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "def extract_flow_features(df):\n",
    "  features_df = df.groupby(['IP_port'], as_index = False).\\\n",
    "    agg({'protocol':['mean', 'count'],\n",
    "         'flow_length': ['mean'],\n",
    "          'pkt_num': ['mean'],\n",
    "         'duration': ['mean']\n",
    "     })\n",
    "    \n",
    "  # rename columns\n",
    "  features_df.columns = ['IP_port', 'protocol', 'flows_num', 'flow_length_mean', 'flow_pkt_num_mean', 'flow_duration_mean']\n",
    "\n",
    "  return features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_reprocessing(df):\n",
    "  df = df.drop(['stream_id', 'protocol', 'length', 'outgoing'], axis=1).reset_index(drop=True)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight(window_indx1, window_indx2):\n",
    "  intersection = window_indx1.intersection(window_indx2)\n",
    "  union = window_indx1.union(window_indx2)\n",
    "  return len(intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge packet-based and flow-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_features(df1, df2):\n",
    "  features_df = pd.merge(df1, df2, on=\"IP_port\")\n",
    "\n",
    "  # sort by complete pkt number\n",
    "  features_df = features_df.sort_values(by=\"complete_pkt_num\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "  return features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function to generate a graph from a traffic chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "input: mobile traffic chunck as a dataframe, N (the maximum nodes kept to build one graph), window (number of seconds used to build weight between two nodes)\n",
    "output: two dataframe. One contains features of all nodes in a graph generated. The other contains weights between the nodes.\n",
    "'''\n",
    "def generate_features_weights(df, N, window):\n",
    "  df = basic_reprocessing(df, N)\n",
    "\n",
    "  #------------------------------ Generate features ------------------------------------\n",
    "  # generate packet-based features\n",
    "  out_df, in_df, complete_df = pkt_reprocessing(df)\n",
    "  complete_df = extract_pkt_features(complete_df)\n",
    "  out_df = extract_pkt_features(out_df, \"out\")\n",
    "  in_df = extract_pkt_features(in_df, \"in\")\n",
    "  pkt_features_df = pd.merge(pd.merge(complete_df, out_df, on=\"IP_port\"), in_df, on=\"IP_port\")\n",
    "  # replace NaN by 0\n",
    "  pkt_features_df = pkt_features_df.fillna(0)\n",
    "  \n",
    "  # generate flow-based features\n",
    "  flow_df = flow_reprocessing(df)\n",
    "  flow_features_df = extract_flow_features(flow_df)\n",
    "\n",
    "  # merge packet-based and flow-based features df into a single features df\n",
    "  features_df = merge_features(pkt_features_df, flow_features_df)\n",
    "\n",
    "  #------------------------------ Generate weights ------------------------------------\n",
    "  w_df = weights_reprocessing(df)\n",
    "  w_df['Time'] = (w_df['Time']//window).astype('int')\n",
    "  w_df = w_df.groupby('IP_port')['Time'].agg(active= lambda x: set(x)).reset_index(drop=False)\n",
    "  \n",
    "  # create a dataframe of weights\n",
    "  destination1_list = []\n",
    "  destination2_list = []\n",
    "  weight_list = []\n",
    "  destinations = list(features_df['IP_port'])\n",
    "  active_destinations = set()\n",
    "\n",
    "  for i in range(len(destinations)):\n",
    "    for j in range(i+1, len(destinations)):\n",
    "      des1 = destinations[i]\n",
    "      des2 = destinations[j]\n",
    "      destination1_list.append(des1)\n",
    "      destination2_list.append(des2)\n",
    "      w = weight(w_df[w_df['IP_port'] == des1]['active'].values[0], w_df[w_df['IP_port'] == des2]['active'].values[0])\n",
    "      weight_list.append(w)\n",
    "      if w > 0:\n",
    "        active_destinations = active_destinations.union({des1, des2})\n",
    "  \n",
    "  # get inactive destinations to remove\n",
    "  inactive_destinations = list(set(destinations) - active_destinations)\n",
    "  \n",
    "  # create dataframe of edge weights\n",
    "  weights_df = pd.DataFrame(\n",
    "  {\n",
    "  \"source\": destination1_list,\n",
    "  \"target\": destination2_list,\n",
    "  \"weight\": weight_list,\n",
    "  }\n",
    "  )\n",
    "\n",
    "  weights_df = weights_df.sort_values(by=\"weight\", ascending=False, ignore_index=True)\n",
    "\n",
    "  # remove destinations that do not connect to any other destinations from features df\n",
    "  features_df = features_df[~features_df['IP_port'].isin(inactive_destinations)]\n",
    "  # add ip features\n",
    "  features_df['ip1'] = features_df['IP_port'].apply(lambda x: int(x[0].split('.')[0]))\n",
    "  features_df['ip2'] = features_df['IP_port'].apply(lambda x: int(x[0].split('.')[1]))\n",
    "  features_df['ip3'] = features_df['IP_port'].apply(lambda x: int(x[0].split('.')[2]))\n",
    "  features_df['ip4'] = features_df['IP_port'].apply(lambda x: int(x[0].split('.')[3]))\n",
    "\n",
    "  # remove destinations that do not connect to any other destinations from weights df\n",
    "  weights_df = weights_df[~weights_df['source'].isin(inactive_destinations) & ~weights_df['target'].isin(inactive_destinations)].reset_index(drop=True)\n",
    "\n",
    "  # min-max normalize weights\n",
    "  weights_df['weight'] = (weights_df['weight'] - weights_df['weight'].min())/(weights_df['weight'].max() - weights_df['weight'].min())\n",
    "\n",
    "  return features_df, weights_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and save graphs from the traffic chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['complete_max', 'complete_min', 'complete_mean', 'complete_mad', 'complete_std', 'complete_var', 'complete_skew',\n",
    "       'complete_kurt', 'complete_pkt_num', 'complete_10per', 'complete_20per', 'complete_30per', 'complete_40per', 'complete_50per', \n",
    "        'complete_60per', 'complete_70per', 'complete_80per', 'complete_90per', 'out_max', 'out_min', 'out_mean', 'out_mad', 'out_std',\n",
    "        'out_var', 'out_skew', 'out_kurt', 'out_pkt_num', 'out_10per', 'out_20per', 'out_30per', 'out_40per', 'out_50per', 'out_60per',\n",
    "        'out_70per', 'out_80per', 'out_90per', 'in_max', 'in_min', 'in_mean', 'in_mad', 'in_std', 'in_var', 'in_skew', 'in_kurt', \n",
    "        'in_pkt_num', 'in_10per', 'in_20per', 'in_30per', 'in_40per', 'in_50per', 'in_60per', 'in_70per', 'in_80per', 'in_90per', \n",
    "        'protocol', 'flows_num', 'flow_length_mean', 'flow_pkt_num_mean', 'flow_duration_mean', 'ip1', 'ip2', 'ip3', 'ip4'\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input:\n",
    "- app_src: folder that contain all traffic chunks (samples) of the app that we want to generate graphs\n",
    "- filenames: list of filenames in app_src (training or testing)\n",
    "Output: Generate graphs and save the graphs for all set of parameters (N and window) for just one app.\n",
    "'''\n",
    "def generate_graphs_one_app(app_src, filenames):\n",
    "\n",
    "  feature_columns = ['IP_port'] + features + ['graph_id']\n",
    "  weight_columns = ['source', 'target', 'weight', 'graph_id']\n",
    "  \n",
    "  features_df = pd.DataFrame([], columns=feature_columns)\n",
    "  weights_df = pd.DataFrame([], columns=weight_columns)\n",
    "  graph_id = 0\n",
    "  # ----------------------------------------------------------------------------\n",
    "\n",
    "  # loop over all traffic chunks of one app\n",
    "  for filename in filenames:\n",
    "    path = os.path.join(app_src, filename)\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    df = df.sort_values(by='Time')\n",
    "      \n",
    "    if df.empty:\n",
    "      print('EMPTY')\n",
    "      continue\n",
    "        \n",
    "    df['Time'] = df['Time'] - df['Time'].iloc[0] # get base Time\n",
    "\n",
    "    #------------- generate one graph -----------------\n",
    "    try:\n",
    "      node_data, weights = generate_features_weights(df, N, window)\n",
    "    except:\n",
    "      print('WRONG')\n",
    "      continue\n",
    "      \n",
    "    if weights.shape[0] > 1:\n",
    "      graph_id = graph_id + 1 \n",
    "      node_data['graph_id'] = graph_id\n",
    "      weights['graph_id'] = graph_id\n",
    "\n",
    "      #------------- add one graph into graphs of the app -----------------\n",
    "      features_df = pd.concat([features_df, node_data], ignore_index=True)\n",
    "      weights_df = pd.concat([weights_df, weights], ignore_index=True)\n",
    "      #--------------------------------------------------------------------\n",
    "\n",
    "  return [features_df, weights_df]\n",
    "      \n",
    "  print(\"================================================================END ONE APP================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input:\n",
    "- A set of parameter: Duration and overlap\n",
    "- Index: 0 if we want to generate graphs for training samples, 1 for testing samples \n",
    "'''\n",
    "def generate_graphs(duration, overlap, index=0):\n",
    "\n",
    "  # get train_test information\n",
    "  path = os.path.join(root_path, '%d_%d'%(duration, overlap), 'train_test_info.json')\n",
    "  with open(path, 'r') as f:\n",
    "    train_test_info = json.load(f)\n",
    "      \n",
    "  samples_folder = os.path.join(root_path, '%d_%d'%(duration, overlap), 'samples')\n",
    "\n",
    "  # initial a dictionary containing features and weights of graphs for all apps (app -> (features_df, weights_df))\n",
    "  graphs = dict()\n",
    "\n",
    "  idx = 0\n",
    "  for app in os.listdir(samples_folder):\n",
    "    if app == '.DS_Store':\n",
    "      continue\n",
    "    idx += 1\n",
    "    print('Loading {} ... {}/{}'.format(app, idx, 101))\n",
    "    \n",
    "    app_src = os.path.join(samples_folder, app)\n",
    "    filenames = train_test_info[app][index]\n",
    "\n",
    "    graphs[app] = generate_graphs_one_app(app_src, filenames)\n",
    "  \n",
    "  return graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate graphs for training dataset\n",
    "training_graphs = generate_graphs(duration, overlap, index=0)\n",
    "# generate graphs for testing dataset\n",
    "testing_graphs = generate_graphs(duration, overlap, index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize features of each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean-std of each features in the training dataset\n",
    "\n",
    "# define the initial empty dataframe\n",
    "cols = ['IP_port'] + features + ['graph_id']\n",
    "df = pd.DataFrame([], columns=cols)\n",
    "\n",
    "# loop over train graphs\n",
    "for app in training_graphs.keys():\n",
    "  df_ = training_graphs[app][0]\n",
    "  df = pd.concat([df, df_], axis=0)\n",
    "\n",
    "# save mean and std of all featurs as dictionary\n",
    "mean_std_dic = dict()\n",
    "for feature in df.columns:\n",
    "  if feature not in ['IP_port', 'ip1', 'ip2', 'ip3', 'ip4', 'protocol', 'graph_id']:\n",
    "    mean_std_dic[feature] = (df[feature].mean(), df[feature].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input: A dataframe containing features of nodes in a graph, dictionary contain mean-std of all features\n",
    "Output: A dataframe of features after standardization\n",
    "'''\n",
    "def standardize_features(df, mean_std_dic):\n",
    "  # standardize the features in dataframe\n",
    "  for feature in mean_std_dic.keys():\n",
    "    m, std = mean_std_dic[feature][0], mean_std_dic[feature][1]\n",
    "    df[feature] = (df[feature] - m)/std\n",
    "  \n",
    "    # normalize ip feature\n",
    "    df['ip1'] = df['ip1']/255\n",
    "    df['ip2'] = df['ip2']/255\n",
    "    df['ip3'] = df['ip3']/255\n",
    "    df['ip4'] = df['ip4']/255\n",
    "  \n",
    "  return df\n",
    "\n",
    "# Standardization\n",
    "for app in training_graphs.keys():\n",
    "  training_graphs[app][0] = standardize_features(training_graphs[app][0], mean_std_dic)\n",
    "  testing_graphs[app][0] = standardize_features(testing_graphs[app][0], mean_std_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graphs(graphs, dataset='train_graphs'):\n",
    "  \n",
    "  #----------------------- create folder to save graphs ---------------------\n",
    "  saved_graph_folder = os.path.join(root_path, '%d_%d'%(duration, overlap), dataset)\n",
    "  if not os.path.exists(saved_graph_folder):\n",
    "    os.mkdir(saved_graph_folder)\n",
    "\n",
    "  N_folder = os.path.join(saved_graph_folder, 'N%d'%N)\n",
    "  if not os.path.exists(N_folder):\n",
    "    os.mkdir(N_folder)\n",
    "\n",
    "  window_folder = os.path.join(N_folder, 't%d'%window)\n",
    "  if not os.path.exists(window_folder):\n",
    "    os.mkdir(window_folder)\n",
    "      \n",
    "  for app in graphs.keys():\n",
    "    graph_app_folder = os.path.join(window_folder, app)      \n",
    "    if not os.path.exists(graph_app_folder):\n",
    "      os.mkdir(graph_app_folder)\n",
    "    \n",
    "    '''\n",
    "    Save graphs for the app as two csv files (features.csv and weights.csv)\n",
    "    '''\n",
    "    features_path = os.path.join(graph_app_folder, 'features.csv')\n",
    "    features_df = graphs[app][0]\n",
    "    weights_path = os.path.join(graph_app_folder, 'weights.csv')\n",
    "    weights_df = graphs[app][1]\n",
    "\n",
    "    features_df.to_csv(features_path)\n",
    "    weights_df.to_csv(weights_path)\n",
    "\n",
    "save_graphs(training_graphs, dataset='train_graphs')\n",
    "save_graphs(testing_graphs, dataset='test_graphs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
